{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting With Your Data\n",
    "\n",
    "Suppose you have some text documents (reports, news articles, PDFs, etc.) and want to ask questions related to the contents of those documents. LLMs, given their proficiency in understanding text, are a great tool for this.\n",
    "\n",
    "In this exercise we'll create an application that answers questions about documents using LLMs.\n",
    "\n",
    "Reference: [https://python.langchain.com/docs/use_cases/question_answering/](https://python.langchain.com/docs/use_cases/question_answering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The pipeline for converting raw unstructured data into a QA chain looks like this:\n",
    "1. `Loading`: [Document Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) load our data as LangChain `Documents`\n",
    "2. `Splitting`: [Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/) break `Documents` into text chunks of a specified size\n",
    "3. `Storage`: Storage (e.g., often a [vector store](https://python.langchain.com/docs/modules/data_connection/vectorstores/)) will house and [embed](https://www.pinecone.io/learn/vector-embeddings/) the text chunks\n",
    "4. `Retrieval`: The app retrieves relevant text chunks from storage (e.g., text chunks [with similar embeddings](https://www.pinecone.io/learn/k-nearest-neighbor/) to the input question)\n",
    "5. `Output`: An [LLM](https://python.langchain.com/docs/modules/model_io/models/llms/) produces an answer using a prompt that includes the question and the retrieved data\n",
    "\n",
    "![./images/qa_flow.jpeg](./images/qa_flow.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickstart\n",
    "\n",
    "To give you a sneak preview, the above pipeline can be all be wrapped in a single object: `VectorstoreIndexCreator`. Suppose we want a QA app over [this](https://en.wikipedia.org/wiki/Bigfoot) Wikipedia article about Bigfoot. We can create this in a few lines of code. \n",
    "\n",
    "First let's install some necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wikipedia chromadb tiktoken langchain langchainhub openai python-dotenv scikit-learn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's configure our OpenAI API key. Create a file called `my.env` in your working directory with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=<your API key>\n",
    "```\n",
    "\n",
    "Be sure to replace `<you API key>` with your actual OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(\"my.env\")\n",
    "assert \"OPENAI_API_KEY\" in os.environ, \"OpenAI API Key not set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the `WikipeidaLoader` and our `VectorstoreIndexCreator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = WikipediaLoader(query=\"Bigfoot\", lang=\"en\", load_max_docs=1)\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain just retrieved the Wikipedia article, split it into sizeable chunks, embedded those chunks, and stored them in our vector store. Pretty cool, huh?\n",
    "\n",
    "Now we can ask our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "from textwrap import wrap\n",
    "\n",
    "llm = OpenAI(temperature=1, model=\"gpt-3.5-turbo-instruct\")\n",
    "output = index.query(\"What is Bigfoot?\", llm=llm)\n",
    "for line in wrap(output, width=140):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood LangChain just retrieved relevant text chunks from our vector store and sent it to an OpenAI LLM along with our question. Job done!\n",
    "\n",
    "Before we move on though, here's a few things to try out:\n",
    "- Ask some different questions, or ask it to generate something else entirely, like a song!\n",
    "- Change the temperature parameter and see how it affects the output. What do you notice?\n",
    "- Change the `WikipediaLoader` query and ask about something else. Did you learn any fun facts?\n",
    "- Change the `WikipediaLoader` to use a different language (e.g. `es` for Spanish). Does it still answer in English?\n",
    "\n",
    "Ok, that's great, but how did it do that and how could we customize this for our specific use case? For that, let's take a look at how we can construct this pipeline piece by piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load\n",
    "\n",
    "Specify a `DocumentLoader` to load in your unstructured data as `Documents`. A `Document` is a piece of text (the `page_content`) and associated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"Bigfoot\", lang=\"en\", load_max_docs=1, load_all_available_meta=True)\n",
    "data = loader.load()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go deeper\n",
    "- Browse the data loader integrations [here](https://integrations.langchain.com/).\n",
    "- See further documentation on loaders [here](https://python.langchain.com/docs/modules/data_connection/document_loaders/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Split\n",
    "\n",
    "Split the `Document` into chunks for embedding and vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `RecursiveCharacterTextSplitter` does its best to split the text into chunks around newlines and word breaks, with a maximum chunk size of 500 characters. But can we do better?\n",
    "\n",
    "Wikipedia articles are already broken up into sections, so it would be great if our splits didn't cross those boundaries. Here's how we can do that:\n",
    "\n",
    "First we split the article up into it's component sections and sub-sections. In this case, main sections are denoted using `==` (e.g. `== History ==`) and sub-sections are denoted using `===` (e.g. `=== Indigenous and early records ===`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"==\", \"Section\"), (\"===\", \"Sub-Section\")])\n",
    "section_splits = text_splitter.split_text(data[0].page_content)\n",
    "\n",
    "section_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the `RecursiveCharacterTextSplitter` to split any sections that are still too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(section_splits)\n",
    "\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our article is nicely split along its sections and we removed all of the headings at the same time!\n",
    "\n",
    "### Go deeper\n",
    "\n",
    "- `DocumentSplitters` are just one type of the more generic [`DocumentTransformers`](https://python.langchain.com/docs/modules/data_connection/document_transformers/), which can all be useful in this preprocessing step.\n",
    "- `Context-aware splitters` keep the location (\"context\") of each split in the original `Document`:\n",
    "    - [Markdown files](https://python.langchain.com/docs/use_cases/question_answering/how_to/document-context-aware-QA)\n",
    "    - [Code (py or js)](https://python.langchain.com/docs/integrations/document_loaders/source_code)\n",
    "    - [PDFs](https://python.langchain.com/docs/integrations/document_loaders/grobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Store\n",
    "\n",
    "To be able to look up our document splits, we first need to store them somewhere. The most common way to do this is to embed the contents of each document then store the embedding and document in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go deeper\n",
    "- Browse the vector store integrations [here](https://integrations.langchain.com/vectorstores).\n",
    "- See further documentation on vector stores [here](https://python.langchain.com/docs/modules/data_connection/vectorstores/).\n",
    "- Browse the text embedding integrations [here](https://integrations.langchain.com/embeddings).\n",
    "- See further documentation on embedding models [here](https://python.langchain.com/docs/modules/data_connection/text_embedding/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Retrieve\n",
    "\n",
    "Retrieve relevant splits for any question using [similarity search](https://www.pinecone.io/learn/what-is-similarity-search/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Did Native Americans believe in Bigfoot?\"\n",
    "docs = vector_store.similarity_search(question, k=1)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go deeper\n",
    "\n",
    "Vector stores are commonly used for retrieval, but they are not the only option. For example, SVMs (Support Vector Machines) can also be used.\n",
    "\n",
    "LangChain [has many retrievers](https://python.langchain.com/docs/integrations/retrievers) including, but not limited to, vector stores. All retrievers implement a common method `get_relevant_documents()` (and its asynchronous variant `aget_relevant_documents()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "\n",
    "svm_retriever = SVMRetriever.from_documents(all_splits, OpenAIEmbeddings())\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common ways to improve on vector similarity search include:\n",
    "- `MultiQueryRetriever` [generates variants of the input question](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever) to improve retrieval.\n",
    "- `Max marginal relevance` selects for [relevance and diversity](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) among the retrieved documents.\n",
    "- Documents can be filtered during retrieval using [`metadata` filters](https://python.langchain.com/docs/use_cases/question_answering/how_to/document-context-aware-QA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=svm_retriever, llm=ChatOpenAI(temperature=0))\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Output\n",
    "\n",
    "Distill the retrieved documents into an answer using an LLM/Chat model (e.g., `gpt-3.5-turbo`) with `RetrievalQA` chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from textwrap import wrap\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=svm_retriever)\n",
    "\n",
    "output = qa_chain({\"query\": question})\n",
    "for line in wrap(output[\"result\"], width=140):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you can pass in an `LLM` or a `ChatModel` (like we did here) to the `RetrievalQA` chain. Don't worry about it too much for now, Chat Models just have a more conversational tone. They also support a different input format which we will get to later.\n",
    "\n",
    "### Go deeper\n",
    "\n",
    "#### Choosing LLMs\n",
    "- Browse the LLM integrations [here](https://integrations.langchain.com/llms).\n",
    "- Browse the Chat Model integrations [here](https://integrations.langchain.com/chat-models).\n",
    "- See further documentation on LLMs and chat models [here](https://python.langchain.com/docs/modules/model_io/models/).\n",
    "- See a guide on local LLMS [here](https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa).\n",
    "\n",
    "#### Customizing the prompt\n",
    "\n",
    "The prompt in RetrievalQA chain can be easily customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from textwrap import wrap\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=svm_retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "output = qa_chain({\"query\": question})\n",
    "for line in wrap(output[\"result\"], width=140):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also store and fetch prompts from the LangChain prompt hub.\n",
    "\n",
    "For example, [here](https://smith.langchain.com/hub/rlm/rag-prompt) is a common prompt for RAG which we can load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "QA_CHAIN_PROMPT_HUB = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=svm_retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT_HUB}\n",
    ")\n",
    "\n",
    "output = qa_chain({\"query\": question})\n",
    "for line in wrap(output[\"result\"], width=140):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return source documents\n",
    "\n",
    "The full set of retrieved documents used for answer distillation can be returned using `return_source_documents=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=svm_retriever,\n",
    "                                       return_source_documents=True)\n",
    "result = qa_chain({\"query\": question})\n",
    "result['source_documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing retrieved document processing\n",
    "\n",
    "Retrieved documents can be fed to an LLM for answer distillation in a few different ways.\n",
    "\n",
    "`stuff`, `refine`, `map-reduce`, and `map-rerank` chains for passing documents to an LLM prompt are well summarized [here](https://python.langchain.com/docs/modules/chains/document/).\n",
    " \n",
    "`stuff` is commonly used because it simply \"stuffs\" all retrieved documents into the prompt.\n",
    "\n",
    "We can pass `chain_type` to `RetrievalQA` to try them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=svm_retriever, chain_type=\"refine\")\n",
    "\n",
    "output = qa_chain({\"query\": question})\n",
    "for line in wrap(output[\"result\"], width=140):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, users have many levels of abstraction to choose from for QA:\n",
    "\n",
    "![./images/summary_chains.png](./images/summary_chains.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But Wait, There's More!\n",
    "\n",
    "We said we were going to teach you to chat with your data, not just ask it questions! Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about.\n",
    "\n",
    "Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatBots\n",
    "\n",
    "With a plain chat model, we can get chat completions by passing one or more messages to the model. The chat model will respond with a message in kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "chat([HumanMessage(content=\"Translate this sentence from English to French: I love Bigfoot.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we pass in a list of messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love Bigfoot.\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then wrap our chat model in a ConversationChain, which has built-in memory for remembering past user inputs and model outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain  \n",
    "  \n",
    "conversation = ConversationChain(llm=chat)  \n",
    "conversation.run(\"Translate this sentence from English to French: I love Bigfoot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.run(\"Now translate it to German.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is `ConversationBufferMemory`:\n",
    "- This memory allows for storing of messages in a `buffer`\n",
    "- When called in a chain, it returns all of the messages it has stored\n",
    "\n",
    "LangChain comes with many other types of memory, too. See [here](https://python.langchain.com/docs/modules/memory/) for in-depth documentation on memory types.\n",
    "\n",
    "For now let's take a quick look at `ConversationBufferMemory`. We can manually add a few chat messages to the memory like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"hi!\")\n",
    "memory.chat_memory.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can load from our memory. The key method exposed by all `Memory` classes is `load_memory_variables`. This takes in any initial chain input and returns a list of memory variables which are added to the chain input.\n",
    "\n",
    "Since this simple memory type doesn't actually take into account the chain input when loading memory, we can pass in an empty input for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also keep a sliding window of the most recent `k` interactions using `ConversationBufferWindowMemory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConversationSummaryMemory` is an extension of this theme. It creates a summary of the conversation over time.\n",
    "\n",
    "This memory is most useful for longer conversations where the full message history would consume many tokens and might not fit into the LLMs context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "memory.save_context({\"input\": \"hi\"},{\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"im working on better docs for chatbots\"},{\"output\": \"oh, that sounds like a lot of work\"})\n",
    "memory.save_context({\"input\": \"yes, but it's worth the effort\"},{\"output\": \"agreed, good docs are important!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConversationSummaryBufferMemory` extends this a bit further. It uses token length rather than number of interactions to determine when to flush interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation\n",
    "\n",
    "We can unpack what goes under the hood with `ConversationChain`.\n",
    "\n",
    "We can specify our memory, `ConversationSummaryMemory` and we can specify the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Prompt \n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"Translate this sentence from English to French: I love Bigfoot.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation({\"question\": \"Now translate the sentence to German.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! But how do I ask it about my Wikipedia article?\n",
    "\n",
    "### Chat Retrieval\n",
    "\n",
    "Combining chat with document retrieval is a popular use case. It allows us to chat with specific information that the model was not trained on.\n",
    "\n",
    "Let's create our memory, as before, but's let's use `ConversationSummaryMemory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the retriever we set up earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=svm_retriever, memory=memory)\n",
    "\n",
    "qa(\"What is Bigfoot?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa(\"Give me a description of one of the sightings. What happened?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just that easy! But wait, what if I want the chat bot to have access to multiple data stores and choose the correct ony dynamically?\n",
    "\n",
    "### Agents\n",
    "\n",
    "The core idea of agents is to use an LLM to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
    "\n",
    "Agents, such as the [conversational retrieval agent](https://python.langchain.com/docs/use_cases/question_answering/how_to/conversational_retrieval_agents), can be used for retrieval when necessary while also holding a conversation.\n",
    "\n",
    "First we create the retriever tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    svm_retriever, \n",
    "    \"search_bigfoot\",\n",
    "    \"Searches and returns documents regarding Bigfoot.\"\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "\n",
    "agent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True)\n",
    "\n",
    "output = agent_executor({\"input\": \"hi, im bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor({\"input\": \"whats my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent_executor({\"input\": \"What color are Bigfoot's eyes?\"})\n",
    "\n",
    "output[\"output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
